{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we specify which directory we're working with. If you only have a tokens file and no documents file, run `tokens-to-documents.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "path = 'cocktails'\n",
    "documents_path = join(path, 'documents')\n",
    "corpus_path = join(path, 'corpus.mm')\n",
    "dictionary_path = join(path, 'dictionary.dict')\n",
    "vectors_path = join(path, 'vectors')\n",
    "words_path = join(path, 'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import isfile, join\n",
    "from progressbar import ProgressBar\n",
    "def listdir(path):\n",
    "    pbar = ProgressBar()\n",
    "    for filename in pbar(os.listdir(path)):\n",
    "        filepath = join(path, filename)\n",
    "        if isfile(filepath) and not filepath.startswith('.'):\n",
    "            yield filepath\n",
    "\n",
    "import nltk\n",
    "def tokenize(document):\n",
    "    return nltk.word_tokenize(document)\n",
    "\n",
    "def load_tokens(path):\n",
    "    for file_path in listdir(path):\n",
    "        with open(file_path, encoding='ascii', errors='ignore') as f:\n",
    "            yield tokenize(f.read().replace('\\n', ' '))\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "def load_dictionary(path):\n",
    "    return corpora.Dictionary(load_tokens(path))\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "def clean_dictionary(dictionary):\n",
    "    stoplist = stopwords.words('english')\n",
    "    stop_ids = [dictionary.token2id[stopword] for stopword in stoplist if stopword in dictionary.token2id]\n",
    "    once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq == 1]\n",
    "    dictionary.filter_tokens(stop_ids + once_ids)\n",
    "    dictionary.compactify()\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.dictionary = clean_dictionary(load_dictionary(path))\n",
    "    def __iter__(self):\n",
    "        for tokens in load_tokens(self.path):\n",
    "            yield self.dictionary.doc2bow(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next line will create the dictionary, which can take a little while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (665 of 665) |#######################| Elapsed Time: 0:00:00 Time: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(documents_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how long the first 10 documents are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "2\n",
      "4\n",
      "7\n",
      "2\n",
      "2\n",
      "4\n",
      "3\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0% (0 of 665) |                        | Elapsed Time: 0:00:00 ETA:  --:--:--\r",
      "  1% (7 of 665) |                         | Elapsed Time: 0:00:00 ETA:  0:00:00"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "for document in islice(corpus, 10):\n",
    "    print(len(document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what the first few documents look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['whiskey', 'cherry_brandy', 'lemon']\n",
      "['gin', 'lemon_juice', 'sugar', 'clear_soda']\n",
      "['gin', 'root_beer']\n",
      "['mandarin_vodka', 'clear_soda', 'sprite', 'cranberry_juice']\n",
      "['orange_liqueur', 'peach_liqueur', 'almond_liqueur', 'vodka', 'pineapple_juice', 'cranberry_juice', 'grenadine']\n",
      "['tequila', 'water']\n",
      "['dark_rum', 'coffee']\n",
      "['gin', 'dry_vermouth', 'sweet_vermouth', 'orange_liqueur']\n",
      "['chocolate_liqueur', 'apricot_brandy', 'heavy_cream']\n",
      "['spiced_rum', 'orange_liqueur', 'raspberry_liqueur', 'ice']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0% (0 of 665) |                        | Elapsed Time: 0:00:00 ETA:  --:--:--\r",
      "  1% (7 of 665) |                         | Elapsed Time: 0:00:00 ETA:  0:00:00"
     ]
    }
   ],
   "source": [
    "for document in islice(load_tokens(documents_path), 10):\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save corpus and dictionary to disk. This may also take a while because it means tokenizing the whole corpus again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (665 of 665) |#######################| Elapsed Time: 0:00:00 Time: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "corpora.MmCorpus.serialize(corpus_path, corpus)\n",
    "corpus.dictionary.save(dictionary_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load corpus and dictionary from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = corpora.MmCorpus(corpus_path)\n",
    "corpus.dictionary = corpora.Dictionary.load(dictionary_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a TF-IDF model from corpus and create transformed corpus. This will discount common terms so they don't affect the LDA as much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = gensim.models.tfidfmodel.TfidfModel(corpus, normalize=True)\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build LDA model from the TF-IDF vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda = gensim.models.ldamodel.LdaModel(corpus_tfidf, id2word=corpus.dictionary,\n",
    "                                      passes=10, # default 1\n",
    "                                      num_topics=10, # default 100\n",
    "                                      iterations=50) # default 50\n",
    "corpus_lda = lda[corpus_tfidf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the data looks like as it's being transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus: [(24, 1.0), (97, 1.0), (99, 1.0)]\n",
      "corpus_tfidf: [(24, 0.6819840625308929), (97, 0.4953692698520294), (99, 0.5380585701762647)]\n",
      "corpus_lda: [(0, 0.036839071098256938), (1, 0.036827188300655796), (2, 0.036832275441643261), (3, 0.036827904548863269), (4, 0.036827910485565288), (5, 0.66853834793986533), (6, 0.036826825691749443), (7, 0.036826825507673189), (8, 0.036826825466830061), (9, 0.036826825518897301)]\n",
      "corpus_lda: [(0, 0.48357878072298816), (1, 0.033776611025191881), (2, 0.033781978411744927), (3, 0.033777386019390752), (4, 0.033776375900691304), (5, 0.033777784853131813), (6, 0.03378063618172384), (7, 0.24618999659378574), (8, 0.033775334200211213), (9, 0.033785116091140252)]\n",
      "corpus_lda: [(0, 0.044659340119688501), (1, 0.044640112406898795), (2, 0.044639699471707102), (3, 0.044638364089912431), (4, 0.59822126693192257), (5, 0.044638209711270055), (6, 0.044641970577112368), (7, 0.044641936756351322), (8, 0.044638207867809865), (9, 0.044640892067326958)]\n",
      "corpus_lda: [(0, 0.033965480582791176), (1, 0.033967725221855601), (2, 0.033967789242242245), (3, 0.033967006111844771), (4, 0.03396585344848832), (5, 0.033965920148291966), (6, 0.033965908571494993), (7, 0.69429400798047547), (8, 0.03396548058793801), (9, 0.033974828104577533)]\n",
      "corpus_lda: [(0, 0.027886317447906036), (1, 0.027889654184729228), (2, 0.027886218880908486), (3, 0.027887325125213157), (4, 0.027883393024702493), (5, 0.027883937634363691), (6, 0.027884790103857728), (7, 0.027884820474906778), (8, 0.027886373205592691), (9, 0.74902716991781981)]\n",
      "corpus_lda: [(0, 0.042048419869396635), (1, 0.042061311771161583), (2, 0.39846762696222243), (3, 0.042048420186165038), (4, 0.042050357439163306), (5, 0.042049504657856819), (6, 0.042048420029225396), (7, 0.26512316857002372), (8, 0.042054350537327638), (9, 0.042048419977457285)]\n",
      "corpus_lda: [(0, 0.041426109017036694), (1, 0.041431562732287319), (2, 0.041426109051008887), (3, 0.04142610906707013), (4, 0.04142612546394301), (5, 0.041426109842699767), (6, 0.34008777445013666), (7, 0.041426129901609952), (8, 0.32849786141088855), (9, 0.041426109063319061)]\n",
      "corpus_lda: [(0, 0.69446626777324028), (1, 0.033948103483441378), (2, 0.033945316776927338), (3, 0.033951902822932728), (4, 0.033945640550311934), (5, 0.033947801815346582), (6, 0.033945276873093767), (7, 0.03395234321442861), (8, 0.033948627380508835), (9, 0.033948719309768373)]\n",
      "corpus_lda: [(0, 0.036771598505448283), (1, 0.036772950814368051), (2, 0.036771598551796993), (3, 0.27797710981029472), (4, 0.036771598679467284), (5, 0.03677438178095039), (6, 0.036771849715700779), (7, 0.036772421060890348), (8, 0.42784489252185426), (9, 0.036771598559228895)]\n",
      "corpus_lda: [(0, 0.034018869331098753), (1, 0.034019609655788437), (2, 0.034027271045778566), (3, 0.034035572748062314), (4, 0.034017552538423111), (5, 0.034019268696117683), (6, 0.25388493699903208), (7, 0.4739271654419443), (8, 0.034029970435570794), (9, 0.034019783108183904)]\n"
     ]
    }
   ],
   "source": [
    "for document in islice(corpus, 1):\n",
    "    print('corpus:', document)\n",
    "for document in islice(corpus_tfidf, 1):\n",
    "    print('corpus_tfidf:', document)\n",
    "for document in islice(corpus_lda, 10):\n",
    "    print('corpus_lda:', document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we serialize all LDA vectors to disk in tsv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numpy_matrix = gensim.matutils.corpus2dense(corpus_lda, num_topics) \n",
    "vectors = numpy_matrix.transpose()\n",
    "import numpy\n",
    "numpy.savetxt(vectors_path, vectors, fmt='%.5g', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want a label for each line of the vectors file, this piece of code will output a `words` file that has a label for each line (based on the filename of the document that generated that line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0% (0 of 665) |                        | Elapsed Time: 0:00:00 ETA:  --:--:--\r",
      "  1% (7 of 665) |                         | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      "  2% (14 of 665) |                        | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      "  3% (21 of 665) |                        | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      "  4% (28 of 665) |#                       | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      "  5% (35 of 665) |#                       | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      "  6% (42 of 665) |#                       | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      "  7% (49 of 665) |#                       | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      "  8% (56 of 665) |##                      | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      "  9% (63 of 665) |##                      | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 10% (70 of 665) |##                      | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 11% (77 of 665) |##                      | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 12% (84 of 665) |###                     | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 13% (91 of 665) |###                     | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 14% (98 of 665) |###                     | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 15% (105 of 665) |###                    | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 16% (112 of 665) |###                    | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 17% (119 of 665) |####                   | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 18% (126 of 665) |####                   | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 20% (133 of 665) |####                   | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 21% (140 of 665) |####                   | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 22% (147 of 665) |#####                  | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 23% (154 of 665) |#####                  | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 24% (161 of 665) |#####                  | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 25% (168 of 665) |#####                  | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 26% (175 of 665) |######                 | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 27% (182 of 665) |######                 | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 28% (189 of 665) |######                 | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 29% (196 of 665) |######                 | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 30% (203 of 665) |#######                | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 31% (210 of 665) |#######                | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 32% (217 of 665) |#######                | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 33% (224 of 665) |#######                | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 34% (231 of 665) |#######                | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 35% (238 of 665) |########               | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 36% (245 of 665) |########               | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 37% (252 of 665) |########               | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 38% (259 of 665) |########               | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 40% (266 of 665) |#########              | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 41% (273 of 665) |#########              | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 42% (280 of 665) |#########              | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 43% (287 of 665) |#########              | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 44% (294 of 665) |##########             | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 45% (301 of 665) |##########             | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 46% (308 of 665) |##########             | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 47% (315 of 665) |##########             | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 48% (322 of 665) |###########            | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 49% (329 of 665) |###########            | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 50% (336 of 665) |###########            | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 51% (343 of 665) |###########            | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 52% (350 of 665) |############           | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 53% (357 of 665) |############           | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 54% (364 of 665) |############           | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 55% (371 of 665) |############           | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 56% (378 of 665) |#############          | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 57% (385 of 665) |#############          | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 58% (392 of 665) |#############          | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 60% (399 of 665) |#############          | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 61% (406 of 665) |##############         | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 62% (413 of 665) |##############         | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 63% (420 of 665) |##############         | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 64% (427 of 665) |##############         | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 65% (434 of 665) |###############        | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 66% (441 of 665) |###############        | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 67% (448 of 665) |###############        | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 68% (455 of 665) |###############        | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 69% (462 of 665) |###############        | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 70% (469 of 665) |################       | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 71% (476 of 665) |################       | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 72% (483 of 665) |################       | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 73% (490 of 665) |################       | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 74% (497 of 665) |#################      | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 75% (504 of 665) |#################      | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 76% (511 of 665) |#################      | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 77% (518 of 665) |#################      | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 78% (525 of 665) |##################     | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 80% (532 of 665) |##################     | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 81% (539 of 665) |##################     | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 82% (546 of 665) |##################     | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 83% (553 of 665) |###################    | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 84% (560 of 665) |###################    | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 85% (567 of 665) |###################    | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 86% (574 of 665) |###################    | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 87% (581 of 665) |####################   | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 88% (588 of 665) |####################   | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 89% (595 of 665) |####################   | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 90% (602 of 665) |####################   | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 91% (609 of 665) |#####################  | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 92% (616 of 665) |#####################  | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 93% (623 of 665) |#####################  | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 94% (630 of 665) |#####################  | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 95% (637 of 665) |###################### | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 96% (644 of 665) |###################### | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 97% (651 of 665) |###################### | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      " 98% (658 of 665) |###################### | Elapsed Time: 0:00:00 ETA:  0:00:00\r",
      "100% (665 of 665) |#######################| Elapsed Time: 0:00:00 Time: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "with open(words_path, 'w') as file:\n",
    "    for path in listdir(documents_path):\n",
    "        document_id = path.split('/')[2].split('.')[0]\n",
    "        file.write(document_id + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
